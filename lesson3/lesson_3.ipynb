{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разработка потоков данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На предыдущем уроке мы написали свой первый DAG, научились описывать, запускать и мониторить пайплайн данных. Познакомились с UI и увидели, как таски меняют свой статус по мере выполнения пайплайна:\n",
    "\n",
    "<img src=\"images/3/task_lifecycle_diagram.png\" style=\"width: 1000px;\"> \n",
    "\n",
    "Впрочем, рассмотренная задача была далека от реальных задач продакшена, как по сложности данных и операций на ними, так и по используемым источникам и инструментам Airflow. Как вы заметили, статусы тасок быстро прошли по пути:\n",
    "\n",
    "1. No status (scheduler создал пустой TaskInstance)\n",
    "2. Scheduled (scheduler определил, что TaskInstance нужно запустить)\n",
    "3. Queued (scheduler отправил task в executor на запуск в порядке очереди)\n",
    "4. Running (worker взял в работу task и она выполняется)\n",
    "5. Success (task'а успешно завершена)\n",
    "\n",
    "<img src=\"images/3/task_stages.png\" style=\"width: 500px;\"> \n",
    "\n",
    "Копнем глубже - познакомимся с еще бОльшим количеством возможностей Airflow и рассмотрим более приближенный к реальности кейс. Однако, прежде чем перейти к рассмотрению самого кейса, разберем еще несколько важных возможностей и особенностей Airflow. В Airflow много инструментов, которые в совокупности позволяют осуществлять тонкую настройку пайплайнов в самых разных аспектах: особенности выполнения задач, параметризация расчетов, расширение базового функционала (кастомизация) и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Больше возможностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущих уроках мы уже познакомились с несколькими объектами Airflow, которые выполняют наши рабочие задачи: Operators и их разновидность Sensors, теперь разберем еще несколько важных приемов.\n",
    "\n",
    "---\n",
    "* __Переменные (Variables)__\n",
    "\n",
    "[Variables](https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html) - пары \"key-value\", хранимые в мета-БД Airflow. Они используются для хранения и получения произвольной информации из метабазы, это могут быть, например, параметры конфигурации или список таблиц. Работать с Variables (создавать, обновлять, удалять) можно через UI (Admin -> Variables), где можно прописать пары \"key-value\" явно или загрузить json-файл, а также можно использовать [airflow variables](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html?highlight=variables#variables) в CLI. Примеры с Variables можно посмотреть [здесь](https://www.applydatascience.com/airflow/airflow-variables/).\n",
    "\n",
    "Получить Variable в коде DAG'а можно так:\n",
    ">from airflow.models import Variable\n",
    "<br>foo = Variable.get(\"foo\")\n",
    "\n",
    "В шаблоне так:\n",
    ">echo {{ var.value.\\<variable_name\\> }}\n",
    "\n",
    "---\n",
    "* __Пулы (Pools)__\n",
    "\n",
    "Пулы - способ объединения tasks в группы для ограничения параллельного выполнения задач, т.к. это может давать сильную нагрузку на систему, подробнее о контроле параллелизма можно прочитать [здесь](https://airflow.apache.org/docs/apache-airflow/stable/faq.html#how-can-my-airflow-dag-run-faster). Список пулов можно настроить в UI (Menu -> Admin -> Pools), там же можно указать сколько слотов воркеров можно использовать для тасок данного пула. Для связи таска с пулом нужно использовать параметр pool, например, при создании инстанса оператора - BashOperator(pool='name', ...). Если задача не связана с пулом, то она относится к default_pool, у которого по умолчанию 128 слотов.\n",
    "\n",
    "---\n",
    "* __Хуки (Hooks)__\n",
    "\n",
    "Хуки - это внешние интерфейсы для работы с различными сервисами: базы данных, внешние API ресурсы, распределенные хранилища типа S3, redis, memcached и т.д. Хуки являются строительными блоками операторов и берут на себя всю логику по взаимодействию с хранилищем конфигов и доступов. Используя хуки можно забыть про головную боль с хранением секретной информации в коде (пароли к доступам, например). Еще раз, отличие хука от оператора: оператор позволяет создавать таски, которые могут (или нет) работать с внешними сервисами, хуки предоставляют унифицированный интерфейс для доступа ко внешним сервисам. Например, [HiveCliHook](https://github.com/apache/airflow/blob/5127ea34e110891c56e1ba9f70211091d13fa553/airflow/hooks/hive_hooks.py) - обертка над [Hive CLI](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli).\n",
    "\n",
    "---\n",
    "* __Подключения (Connections)__\n",
    "\n",
    "[Connections](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html) позволяют Airflow подключаться к внешним системам согласно указанным credentials: хост/порт/логин/пароль; идентификатором подключения является __conn_id__. Многие хуки имеют дефолтное значение conn_id, и когда операторы используют такие хуки, не нужно указывать явно connection ID, например, для PostgresHook conn_id=postgres_default. \n",
    "\n",
    "Добавлять и настраивать connections можно разными способами:\n",
    "\n",
    "- через UI Admin -> Connections\n",
    "- через CLI и аргументы: список существующих (airflow connections list), добавить новый (airflow connections add 'name' --conn-host 'host' ...)\n",
    "- через CLI и json/yaml/env: экспорт из json-файла (airflow connections export connections.json)\n",
    "- через переменные окружения: формат имени AIRFLOW_CONN_\\{CONN_ID\\}, например, AIRFLOW_CONN_MY_DATABASE='my-conn-type://login:password@host:port/schema?param1=val1&param2=val2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* __Плагины (Plugins)__\n",
    "\n",
    "[Плагины](https://airflow.apache.org/docs/apache-airflow/stable/plugins.html) позволяют расширить возможности Airflow путем добавления кастомного функционала. В плагин могут входить операторы, хуки, сенсоры и методы, которые разработчик определил в модуле плагина, помещенного в папку __$AIRFLOW_HOME/plugins__:\n",
    "<img src=\"images/3/airflow_home.png\" style=\"width: 200px;\"> \n",
    "\n",
    "Плагин наследует базовый класс AirflowPlugin и переопределяет поля и методы:\n",
    "<img src=\"images/3/plugin.png\" style=\"width: 300px;\"> \n",
    "\n",
    "При добавлении/обновлении плагинов нужно перезапустить webserver, либо в секции \\[webserver\\] airflow.cfg указать __reload_on_plugin_change=True__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* __Еще больше возможностей__\n",
    "\n",
    "1. [Тэгирование DAG'ов для фильтрации в UI](https://airflow.apache.org/docs/apache-airflow/stable/howto/add-dag-tags.html)\n",
    "2. [Настройка цветов состояний в UI](https://airflow.apache.org/docs/apache-airflow/stable/howto/customize-state-colors-ui.html)\n",
    "3. Airflow в связке с [proxy](https://airflow.apache.org/docs/apache-airflow/stable/howto/run-behind-proxy.html), [systemd](https://airflow.apache.org/docs/apache-airflow/stable/howto/run-with-systemd.html) или [upstart](https://airflow.apache.org/docs/apache-airflow/stable/howto/run-with-systemd.html)\n",
    "4. [Cсылка на внешний ресурс в операторе](https://airflow.apache.org/docs/apache-airflow/stable/howto/define_extra_link.html)\n",
    "5. [Дополнения от провайдеров](https://airflow.apache.org/docs/apache-airflow-providers/index.html)\n",
    "6. [Ветвление в DAG'е](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#branching)\n",
    "7. [SLA пайплайнов](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#slas)\n",
    "8. [Trigger Rules](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#trigger-rules) - для более сложных зависимостей между тасками\n",
    "9. [LatestOnlyOperator](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#latest-run-only) для пропуска тасок, которые не относятся к последнему запуску DAG'а.\n",
    "10. [\"Мертвые\" таски](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#zombies-undeads)\n",
    "11. [Документирование DAG/task для отображения в UI](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#documentation-notes)\n",
    "12. [Про безопасность](https://airflow.apache.org/docs/apache-airflow/stable/security/index.html)\n",
    "13. [Управление модулями](https://airflow.apache.org/docs/apache-airflow/stable/modules_management.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Особенности execution_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим также особенности работы с датами, Airflow разрабатывался как инструмент для решения задач, связанных с обработкой данных. В этом мире мы обычно обрабатываем крупную порцию данных только тогда, когда она готова, то есть на следующий день. И создатели Airflow изначально изложили такую концепцию в своих продуктах:\n",
    "\n",
    "<img src=\"images/3/execution_date.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда мы __сегодня__ запускаем ежедневный пайплайн, то с большой вероятностью захотим обрабатывать данные за __вчера__. Именно поэтому execution_date будет равен левой границе интервала, за которой мы обрабатываем данные. Например, сегодняшний запуск, который стартовал в час ночи по UTC, получит в качестве execution_date вчерашнюю дату. В случае ежечасного пайплайна ситуация такая же: для запуска пайплайна в 6 утра время в execution_date будет равно 5 часам утра. Это мысль поначалу не очень очевидна, но тем не менее, она очень осмысленная и важная.\n",
    "\n",
    "__Еще раз__: scheduler запускает задачу спустя один schedule_interval после execution_date в конце интервала, то есть задача с execution_date='2019-11-21' и schedule_interval=timedelta(days=1) запустится сразу после 2019-11-21 23:59:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важный момент с терминологией: в контексте Airflow слово __previous__ относится к запуску с __предыдущей execution_date__, который независим от других запусков, а слово __upstream__ относится к зависимостям внутри одного запуска и с __одинаковой execution_date__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перезапуск DAG'а"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом уроке мы познакомились с Task Actions в TaskInstance для перезапуска тасок и изменения статуса (Run, Clear, Mark Failed/Success), но помимо этого в Airflow есть еще 2 механизма, относящихся к запуску прошлых задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* __[Catchup](https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#catchup)__\n",
    "\n",
    "Заданные start_date, end_date и schedule_interval определяют набор DAG Runs, которые scheduler будет запускать. При этом по умолчанию scheduler будет последовательно выполнять все DAG Runs с момента последней execution date. Это называется __Catchup__.\n",
    "\n",
    "Изменить эту логику можно указав в коде DAG'а: _dag = DAG(..., catchup=False)_, либо в конфиге: _catchup_by_default = False_, тогда scheduler будет создавать DAG run только для последнего интервала.\n",
    "\n",
    "---\n",
    "* __[Backfill](https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#backfill)__\n",
    "\n",
    "Процесс создания инстансов DAG'а (DAG Run) для указанного исторического периода называется __Backfill__. Если start_date в прошлом, то с шагом schedule_interval Airflow сделает Backfill и создаст DAG Run'ы, которые не обязательно запускать (catchup=False). Иногда бывает нужно создать DAG Run'ы для указанного исторического периода до start_date и при этом, например, отметить все запуски как успешные (-m) в мета-БД, это можно сделать командой:\n",
    "\n",
    ">airflow dags backfill -m -s 2020-12-01 -e 2020-12-28 first_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubDAGs и TaskGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubDAGs - разделяем особенности выполнения частей DAG'а"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию, для всех DAG-графов предполагается только один конкретный executor из-за того, что на одном рабочем узле запускается лишь один исполнитель. В конфигурационном файле (airflow.cfg) это задается следующим образом:\n",
    "\n",
    "> executor = LocalExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, на практике иногда необходимо, чтобы разные DAG’и выполнялись по-разному. Обойти это ограничение можно с помощью подграфа DAG (SubDAG), передав ему исполнителя с помощью оператора __SubDagOperator__. SubDAG'и удобны для повторяющихся схожих тасок. Задавать функцию, возвращающую объект DAG'а - это хороший паттерн в Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subdag(parent_dag_name, child_dag_name, args):\n",
    "    dag_subdag = DAG(\n",
    "        dag_id=f'{parent_dag_name}.{child_dag_name}',\n",
    "        default_args=args,\n",
    "        start_date=days_ago(2),\n",
    "        schedule_interval=\"@daily\",\n",
    "    )\n",
    "\n",
    "    for i in range(5):\n",
    "        DummyOperator(\n",
    "            task_id='{}-task-{}'.format(child_dag_name, i + 1),\n",
    "            default_args=args,\n",
    "            dag=dag_subdag,\n",
    "        )\n",
    "\n",
    "    return dag_subdag\n",
    "\n",
    "section_1 = SubDagOperator(\n",
    "    task_id='section-1',\n",
    "    subdag=subdag(DAG_NAME, 'section-1', args),\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/3/subdag_before.png\" style=\"width: 400px;\">\n",
    "<center>До</center>\n",
    "<img src=\"images/3/subdag_after.png\" style=\"width: 400px;\">\n",
    "<center>После</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же задачи одного DAG-графа не всегда могут быть запланированы на одной машине, то, при разделении их по разным worker’ам, необходимо сохранение состояния задания и его совместное использование. Например, требуется локально выполнить обработку какого-либо файла с использованием BashOperator или PythonOperator. Однако, LocalExecutor выполняет задачу на том же узле, где запущен планировщик, а CeleryExecutor ставит задачи в очередь для обработки с помощью Celery-библиотеки. Обойти такое ограничение можно с помощью общего сетевого хранилища на всех машинах, где работают исполнители. Благодаря этому планировщик и веб-сервер могут совместно использовать папку с DAG-файлами и работать на разных компьютерах. Про особенности работы CeleryExecutor можно прочитать [здесь](https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TaskGroups - визуально группируем таски, не изменяя выполнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таски можно для удобства группировать при отображении в Graph View UI (данная фича появилась в версии 2.0). Если в DAG'е есть объемные повторяющиеся паттерны, например, для каждой категории товаров отдельный таск и так происходит на нескольких шагах пайплайна, то можно таски сгруппировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TaskGroup(\"group1\") as group1:\n",
    "    task1 = DummyOperator(task_id=\"task1\")\n",
    "    task2 = DummyOperator(task_id=\"task2\")\n",
    "\n",
    "task3 = DummyOperator(task_id=\"task3\")\n",
    "\n",
    "group1 >> task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от SubDagOperator, TaskGroup - это только про UI группировку тасок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Области видимости и контекст выполнения task’и"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видимость DAG'а"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow считывает все DAG'и, которые может увидеть в файле DAG'а, то есть DAG должен быть в __globals()__, в глобальной области видимости, то есть из примера ниже только dag_1 будет прочтен Airflow и отображен в UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_1 = DAG('this_dag_will_be_discovered')\n",
    "\n",
    "def my_function():\n",
    "    dag_2 = DAG('but_this_dag_will_not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это не относится к SubDagOperator, так как subdag определяется внутри функции и Airflow не будет пытаться загрузить его как отдельный DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Контекст выполнения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Питоновский скрипт в Airflow, описывающий DAG, это лишь конфигурационный файл для описания структуры DAG'а. Сами таски, которые мы определяем в файле DAG'а запускаются в другом контексте, отличном от контекста данного скрипта. Разные запуски тасок могут проходить на разных воркерах в разные моменты времени, то есть скрипт не может быть использован для кросс-коммуникации между тасками. Для таких целей используется отдельный механизм, называемый __XComs__ (о нем ниже).\n",
    "\n",
    "В общем случае в DAG файле не следует делать какой-то процессинг данных, так как цель данного скрипта лишь определить DAG и его структуру. Этот файл должен выполняться за секунды, так как scheduler будет исполнять его периодически, чтобы отслеживать изменения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XComs - обмен данными между tasks и DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операторы обычно атомарны - они независимы и не нуждаются в делении ресурсов с остальными операторами. DAG определяет лишь порядок исполнения, а сами запуски независимы, они могут быть даже на разных машинах. Если же двум операторам нужно делить информацию, например, имя файла/таблицы или небольшой массив данных, их лучше объединить в один оператор. Если этого не избежать, нужно использовать XComs (\"cross-communication\").\n",
    "\n",
    "XComs по факту определяется ключем (key), значением (value) и таймстемпом (timestamp), но также отслеживает какой task/DAG создал XCom и когда он должен быть доступен. В значение XCom'а можно положить любой объект, который может быть сериализован (pickled), поэтому важно следить за размером объекта. Для записи информации в XComs нужно сделать __\"push\"__, а для получения значения по ключу - __\"pull\"__. Когда таска пушит Xcom, он становится доступен для других тасок. \n",
    "\n",
    "Xcom можно запушить в любое время вызовом метода __xcom_push()__. При этом, если таска возвращает значение (либо из execute() метода Operator'а, либо из python_callable функции PythonOperator'а), то XCom считает такое значение __автоматически запушеным__.\n",
    "\n",
    "Таски могут вызывать __xcom_pull()__ для получения XCom, при этом можно \"подтянуть\" не все XCom, а отфильтровать по ключу (key), исходным task_ids или dag_id. По умолчанию, xcom_pull() фильтрует ключи, которые были автоматически запушены из возвращенных значений функций (в отличие от тех XCom, что запушили вручную через xcom_push()). Если в xcom_pull() передать одну строку с task_id, то вернется самое последнее по времени значение XCom, если лист из task_ids, то вернется соответствующий лист значений XCom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside a PythonOperator called 'pushing_task'\n",
    "def push_function():\n",
    "    return value\n",
    "\n",
    "# inside another PythonOperator\n",
    "def pull_function(task_instance):\n",
    "    # pulling XCom\n",
    "    value = task_instance.xcom_pull(task_ids='pushing_task')\n",
    "    new_value = value * 2\n",
    "    # pushing changed value\n",
    "    task_instance.xcom_push(key=\"name\", value=new_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также возможно сделать pull XCom прямо в шаблон:\n",
    "\n",
    ">SELECT * FROM {{ task_instance.xcom_pull(task_ids='foo', key='table_name') }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XComs похожи на Variables, но были созданы именно для взаимодействия между тасками, а не для глобальных настроек. Также возможно изменить логику XComs по сериализации/десериалиазации результатов таска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алертинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда количество пайплайнов большое и за всеми следить становится трудоемко, уместно настроить механизмы алертинга (англ. alert, предупреждение), которые бы оперативно уведомляли, например, об ошибках. Существует множество вариантов алертинга: сообщение в чат (Slack, Telegram etc.), е-мейл письмо, системы алертинга ([Opsgenie](https://www.atlassian.com/software/opsgenie)) и др.\n",
    "\n",
    "Для примера рассмотрим задачу, в которой мы должны выполнить запрос в Hive и об успешном окончании пайплайна хотим получать уведомление в Slack. Для этого мы можем использовать __HiveOperator__, который создан как раз для того, чтобы отправлять запросы на выполнение в Hive. Для запуска оператора нужно указать название задачи, пайплайн, идентификатор соединения к Hive и выполняемый запрос. Следующей таской в DAG'е сделаем оператор __SlackAPIPostOperator__, который умеет оправлять сообщения в Slack. В таком случае код DAG'а содержал бы следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(\n",
    "        dag_id='dag_name',\n",
    "        default_args=args\n",
    ") as dag:\n",
    "\n",
    "    run_sql = HiveOperator(\n",
    "       dag=dag,\n",
    "       task_id=\"run_sql\",\n",
    "       hive_cli_conn_id=\"hive\",\n",
    "       hql=\"\"\"\n",
    "           INSERT OVERWRITE TABLE target_table\n",
    "           SELECT * FROM table_one t1\n",
    "           JOIN table_two t2 on t1.key = t2.key\n",
    "           WHERE table_one.dt = '{{ ds }}'\n",
    "       \"\"\"\n",
    "    )\n",
    "\n",
    "    notify = SlackAPIPostOperator(\n",
    "       dag=dag,\n",
    "       task_id=\"notify_slack\",\n",
    "       slack_conn_id=\"slack\",\n",
    "       token=token,\n",
    "       channel=\"airflow_alerts\",\n",
    "       text=\"Guys, I'm done for {{ ds }}\"\n",
    "    )\n",
    "\n",
    "    run_sql >> notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы помним, {{ ds }} – это макрос, который возьмет в execution_date только дату в формате %Y-%m-%d. В определенный момент перед запуском оператора Airflow отрендерит строку запроса, подставит туда нужную дату и отправит запрос на выполнение.\n",
    "\n",
    "Подробнее о настройке алертинга Airflow в Slack можно прочитать [здесь](https://www.reply.com/data-reply/en/content/integrating-slack-alerts-in-airflow), а как связать Airflow с алертинг-платформой от Atlassian (разработчик JIRA, Confluence, Bitbucket, Trello и др.) описано [тут](https://medium.com/unruly-engineering/quick-easy-alerting-for-apache-airflow-53c3f1ba2ca)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мониторинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О мониторинге выполнения тасок и просмотре логов уже упоминали на прошлом уроке, но помимо этого Airflow предоставляет [множество возможностей](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/index.html) для логирования, сбора метрик и дальнейшей визуализации в других системах. При этом помимо этого Airflow умеет самостоятельно обнаруживать ошибки в свои операциях с помощью механизма [health check](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/check-health.html). Так как Airflow в основном используется для запуска пайплайнов данных в продакшене, он поддерживает real-time нотификацию об ошибках.\n",
    "\n",
    "С деталями можно ознакомиться здесь:\n",
    "* [Архитектура логирования и мониторинга Airflow](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-architecture.html)\n",
    "* [Логирование тасок](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-tasks.html)\n",
    "* [Сбор метрик](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/metrics.html) в [StatsD](https://github.com/statsd/statsd)\n",
    "* [Отслеживание ошибок](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/errors.html) в [Sentry](https://docs.sentry.io/)\n",
    "* [Мониторинг пользовательской активности](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/tracking-user-activity.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание усложненного DAG'а"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отработки практических навыков создания пайплайнов, рассмотрим усложненную задачу на примере данных из прошлого урока. Мы попробовали скачать данные из сети Интернет, сохранить файл локально, преобразовать его и результат сохранить в итоговый csv файл, плюс потренировались с Bash-оператором.\n",
    "\n",
    "Теперь рассмотрим следующую задачу, необходимо выполнить те же действия с датасетом Titanic, что и в предыдущем уроке, но с рядом отличий:\n",
    "\n",
    "1. Все функции вынесены в отдельный модуль и в DAG файле только сама структура графа (директория модулей должна быть в PATH)\n",
    "2. Отказ от работы с локальными файлами: \n",
    "    - сначала скачанный датасет пушится в XCom (он весит ~50 КБ)\n",
    "    - затем он пуллится из XCom и передается двум преобразованиям (pivot и mean_fare)\n",
    "3. Результаты преобразований записываются в две таблицы локальной базы PostgreSQL (Connections+Hooks или psycopg2/sqlalchemy).\n",
    "4. Имена таблиц в PostgreSQL заданы в Variables\n",
    "\n",
    "Для использования PostgresHook необходимо установить соответствующие модули от provider'а Postgres в окружение conda369:\n",
    "\n",
    ">pip install apache-airflow-providers-postgres\n",
    "\n",
    "Для данной задачи можно создать отдельную БД (например, data_warehouse), создать в ней пользователя airflow и дать ему права (все по аналогии с предыдущим уроком). Напомним несколько основных команд для работы с Postgres (примеры для MacOS, на Ubuntu и др. по аналогии):\n",
    "\n",
    "* посмотреть список БД и пользователей\n",
    "    >psql -l\n",
    "\n",
    "* подключиться к созданной БД data_warehouse под юзером airflow\n",
    "    >psql -U airflow data_warehouse \n",
    "\n",
    "* переключиться на другую БД\n",
    "    >\\c postgres\n",
    "\n",
    "* вывести список всех существующих таблиц в текущей БД\n",
    "    >\\dt\n",
    "    \n",
    "* получить описание существующей таблицы\n",
    "    >\\d my_table\n",
    " \n",
    "Примеры SQL запросов для создания таблицы, вставки значений и запросов к таблице есть в [документации](https://www.postgresql.org/docs/13/tutorial-sql.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow - это гибкий инструмент с множеством настроек: он поддерживает параметризацию, позволяет контролировать параллелизм, умеет подключаться к множеству источников. Также есть возможность добавления кастомных расширений, настройки визуализации в UI, группировки тасок как внешне, так и внутренне. Механизм обмена сообщений между задачами дает дополнительную степень свободы, а алертинг и мониторинг делают инструмент удобным и контролируемым в продакшене.\n",
    "\n",
    "На практике, хорошо, если вы будете использовать хотя бы половину из всего рассмотренного арсенала Airflow, однако быть знакомым со всем  спектром возможностей инструмента крайне важно для Data Engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Начало работы с Apache Airflow](https://www.machinelearningmastery.ru/getting-started-with-apache-airflow-df1aa77d7b1b/)\n",
    "2. [Apache Airflow: делаем ETL проще](https://habr.com/ru/post/512386/)\n",
    "3. [Integrating Slack alerts in Airflow](https://www.reply.com/data-reply/en/content/integrating-slack-alerts-in-airflow)\n",
    "4. [Airflow XComs example](https://big-data-demystified.ninja/2020/04/15/airflow-xcoms-example-airflow-demystified/)\n",
    "5. [Access the Airflow Database](https://www.astronomer.io/docs/cloud/stable/customize-airflow/access-airflow-database)\n",
    "6. [Managing your Connections in Apache Airflow](https://www.astronomer.io/guides/connections)\n",
    "7. [How to use PostgresOperator in Apache Airflow?](https://xnuinside.medium.com/short-guide-how-to-use-postgresoperator-in-apache-airflow-ca78d35fb435)\n",
    "8. [ETL Pipelines With Airflow](http://michael-harmon.com/blog/AirflowETL.html) (пример с PostgreSQL)\n",
    "9. [Building a Data Pipeline with Airflow](https://tech.marksblogg.com/airflow-postgres-redis-forex.html)\n",
    "10. [Simple ETL with Airflow](https://medium.com/python-in-plain-english/simple-etl-with-airflow-372b0109549)\n",
    "11. [How to Write Airflow ETL - Intermediate Tutorial](https://angelddaz.substack.com/p/how-to-write-airflow-etl-intermediate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать новый вариант пайплайна в соответствии с требованиями в разделе _\"Создание усложненного DAG'а\"_.\n",
    "\n",
    "__Формат сдачи д/з__: приложите ссылку на ваш Git с кодом DAG'а и модулей, туда же загрузите скриншоты с:\n",
    "\n",
    "* разделом Variables в UI с названиями таблиц;\n",
    "* разделом XComs в UI с возвращенным значением исходного датасета;\n",
    "* выводом консоли с select * обеих таблиц в PostgreSQL (два результата расчета; строк мало, поэтому на экране поместятся оба селекта)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
