{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Установка AirFlow. Создание и основные параметры DAG. Web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow можно развернуть по-разному: это могут быть как поднятые компоненты Airflow на одной машине вместе с запускаемыми воркерами, а может быть Airflow поднят, как сервис в pod'е кластера Kubernetes, и воркеры будут запускаться на отдельных pod'ах. Второй подход распространен в production среде и хорошо масштабируется, однако он более сложен в настройке и мы рассмотрим его в следующих уроках, а в рамках текущего развернем Airflow на своей локальной машине и напишем свой первый DAG.\n",
    "\n",
    "Для начала вспомним архитектуру Airflow, чтобы понять, что нам предстоит запустить:\n",
    "\n",
    "<img src=\"images/2/arch-diag-basic.png\" style=\"width: 800px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow состоит из нескольких основных частей:\n",
    "\n",
    "* БД для хранения метаданных о пайплайнах (Metadata DB), чаще всего используют PostgreSQL, также встречается MySQL\n",
    "* Веб-приложение (Webserver) с панелью управления, написано на Flask\n",
    "* Планировщик (Scheduler), в production среде чаще всего используется Celery\n",
    "* Исполнитель (Executor) на схеме показан отдельно, как это часто подразумевается в документации, но в реальности это не отдельный процесс, а работающий в рамках Scheduler'а\n",
    "* Воркер (Worker), выполняющий работу, в production среде также чаще всего встречается конфигурация с Celery\n",
    "\n",
    "На схеме также показан airflow.cfg, в котором хранятся настройки Airflow и к которому обращаются Web Server, Scheduler и Workers, а также отображены DAG'и - файлы с кодом на Python, где описаны пайплайны, к ним тоже обращаются остальные компоненты Airflow.\n",
    "\n",
    "Приступим к установке и настройке Apache Airflow руками без использования готовых Docker-образов ([подробнее](https://airflow.apache.org/docs/apache-airflow/stable/production-deployment.html) об образе с Airflow и деплое в проде), чтобы наглядно показать как всё запускается изнутри."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1. Окружение и airflow package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём новое виртуальное окружение Python, и ставим в него Apache Airflow версии 2.0.0:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ python3 -m venv .venv\n",
    "$ source .venv/bin/activate\n",
    "$ pip install apache-airflow==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отключить виртуальное окружение можно с помощью команды __deactivate__.\n",
    "\n",
    "Подводные камни:\n",
    "\n",
    "* С помощью __venv__ нельзя создать окружение с версией Python, отличной от уже установленной в системе\n",
    "* Могут быть проблемы совместимости версий пакетов: так, например, на MacOS с версией Python __3.8.2__ установка Airflow 2.0.0 падает при установке пакета __setproctitle__ с ошибкой _\"ERROR: Could not build wheels for setproctitle which use PEP 517 and cannot be installed directly\"_\n",
    "\n",
    "В случае конфликтов версий, предлагается явно создать окружение с Python __3.6.9__, это можно сделать с помощью __conda__, не удаляя при этом существующую версию Python на компьютере:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ conda create -y -n conda369 python=3.6.9\n",
    "$ conda activate conda369\n",
    "$ pip install apache-airflow==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отключить conda окружение можно с помощью команды __conda deactivate__.\n",
    "\n",
    "У Airflow много зависимостей в отличие от того же Luigi, поэтому в терминале при установке будет много информации. После установки Airflow можете посмотреть зависимости через __pip freeze__.\n",
    "\n",
    "После установки пакета apache-airflow, в виртуальном окружении будет доступна команда airflow. Запустите её без параметров, чтобы увидеть список доступных команд. Например, основную информацию об Airflow можно узнать с помощью команды __airflow info__, а список наиболее полезных комманд - __airflow cheat-sheet__. Подробнее с Airflow CLI можно познакомиться [здесь](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2. БД и airflow.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow свои настройки хранит в файле __airflow.cfg__, который по умолчанию будет создан в домашней директории юзера по пути __~/airflow/airflow.cfg__. В частности, в начале файла указан путь к папке, где будут располагаться DAG'и (dags_folder).\n",
    "\n",
    "Путь можно изменить, присвоив переменной окружения новое значение:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ export AIRFLOW_HOME=~/airflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее выполняем инициализацию для базы данных ([подробнее](https://airflow.apache.org/docs/apache-airflow/stable/howto/initialize-database.html)):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ airflow db init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта команда накатит все миграции, по умолчанию в качестве базы данных Airflow использует SQLite. Для демонстрационных возможностей это нормально, но в реальном бою лучше всё же переключиться на MySQL или PostgreSQL. Будем использовать PostgreSQL, поэтому если он у вас не стоит, то самое время установить PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Установим актуальную версию PostgreSQL: \n",
    "\n",
    "* на MacOS пользуемся пакетным менеджером Homebrew\n",
    "* на Ubuntu отличия минимальные, ориентироваться можно на [шпаргалку](https://khashtamov.com/ru/postgresql-cheatsheet/)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ brew install postgresql\n",
    "$ postgres --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При установке автоматически должна будет создана папка на диске с БД, если этого не произошло, создадим БД сами, а также запустим PostgreSQL сервер (остановить можно с помощью такой же команды с __stop__):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ initdb /usr/local/var/postgres\n",
    "$ pg_ctl -D /usr/local/var/postgres start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Флаг __-D__ означает, что PostgreSQL сервер запустится как демон и будет работать в фоновом режиме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Войдем в интерактивный терминал PostgreSQL, создадим базу данных и пользователя к ней для Airflow:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ psql postgres\n",
    "$ postgres=# CREATE database airflow_metadata;\n",
    "$ postgres=# CREATE user airflow WITH password 'airflow';\n",
    "$ postgres=# GRANT all privileges on database airflow_metadata to airflow;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь открываем airflow.cfg и правим значение параметров:\n",
    "\n",
    "* __sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow_metadata__\n",
    "* __load_examples = False__\n",
    "\n",
    "Последний параметр отвечает за загрузку примеров с DAG'ами, они в общем случае не нужны, хотя сами можете попробовать оставить и посмотреть примеры. Если что-то нечаянно удалите в файле конфига, default версию можно взять в [репозитории Apache Airflow](https://github.com/apache/airflow/blob/master/airflow/config_templates/default_airflow.cfg).\n",
    "\n",
    "В качестве python-драйвера для PostgreSQL используем __psycopg2__, поэтому его необходимо доставить в окружение:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ pip install psycopg2==2.8.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем новую базу данных:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ airflow db init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В стандартной конфигурации Airflow предлагает нам использовать SequentialExecutor, но мы ведь стараемся подражать продуктивной среде, поэтому будем использовать LocalExecutor. В airflow.cfg поменяйте значение параметра __executor__ на __LocalExecutor__.\n",
    "\n",
    "Интересный факт - если в качестве базы метаданных использовать однопоточный SQLite, то LocalExecutor превратится в SequentialExecutor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3. Запуск webserver и scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед запуском Airflow осталось выполнить еще одно действие - создать пользователя:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ airflow users create \\\n",
    "    --username admin \\\n",
    "    --password admin \\\n",
    "    --firstname John \\\n",
    "    --lastname Doe \\\n",
    "    --role Admin \\\n",
    "    --email johndoe@example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем веб-приложение на 8080 порту и логинимся под созданным пользователем (если этот порт у вас занят, укажите другой свободный):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ airflow webserver -p 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все настроено верно, то увидим перед собой интерфейс:\n",
    "\n",
    "<img src=\"images/2/airflow_wo_scheduler.png\" style=\"width: 800px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На странице можно заметить сообщение: \n",
    "\n",
    "_\"The scheduler does not appear to be running. \n",
    "The DAGs list may not update, and new tasks will not be scheduled.\"_\n",
    "\n",
    "Сообщение указывает на то, что не запущен планировщик Airflow - Scheduler. Он отвечает за DAG discovery (обнаружение новых DAG'ов), а также за планирование их запуска. Запустим планировщик командой:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ airflow scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запустить планировщик в отдельном терминале, либо использовать менеджер терминалов __tmux__, например. Из интересного стоит отметить, что можно настроить запуск сервисов Airflow через [systemd](https://github.com/apache/incubator-airflow/tree/master/scripts/systemd) или [docker](https://github.com/puckel/docker-airflow) (примеры из репозитория Airflow).\n",
    "\n",
    "Итак, база настроена, веб-приложение и планировщик запущены. Теперь напишем первый __data pipeline__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание DAG и его параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройка среды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы наверняка обратили внимание, в файле настроек airflow.cfg есть параметр __dags_folder__, он указывает на путь, где лежат файлы с DAG'ами. Это путь \\$AIRFLOW_HOME/dags. Именно туда мы положим наш код с задачами.\n",
    "\n",
    "Писать код предлагается в __PyCharm__, можно создавать и редактировать \\*.py файлы с DAG'ами в отдельном проекте, а в папку \\$AIRFLOW_HOME/dags копировать по мере готовности. Для подсветки синтаксиса модулей Airflow и соответствующей версии Python, нужно в настройках интерпретатора выбрать существующее окружение conda369, которое мы создали ранее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2/pycharm_create.png\" style=\"width: 800px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание и запуск DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для демонстрации возьмем пример с датасетом __Titanic__ из популярного соревнования на [Kaggle](https://www.kaggle.com/c/titanic/overview). Сделаем следующее: скачаем датасет, а затем создадим сводную таблицу - сгруппируем пассажиров по полу и пассажирскому классу, чтобы узнать количество людей в каждом классе. Результатом будет новый csv-файл со сводной таблицей. \n",
    "\n",
    "Для наглядности посмотрим содержимое датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (887, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                             Mr. Owen Harris Braund   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2         1       3                              Miss. Laina Heikkinen   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4         0       3                            Mr. William Henry Allen   \n",
       "\n",
       "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0    male  22.0                        1                        0   7.2500  \n",
       "1  female  38.0                        1                        0  71.2833  \n",
       "2  female  26.0                        0                        0   7.9250  \n",
       "3  female  35.0                        1                        0  53.1000  \n",
       "4    male  35.0                        0                        0   8.0500  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dataset size:', c.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Познакомившись с данными, напишем процессинг датасета и создание DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# базовые аргументы DAG\n",
    "args = {\n",
    "    'owner': 'airflow',  # Информация о владельце DAG\n",
    "    'start_date': dt.datetime(2020, 12, 23),  # Время начала выполнения пайплайна\n",
    "    'retries': 1,  # Количество повторений в случае неудач\n",
    "    'retry_delay': dt.timedelta(minutes=1),  # Пауза между повторами\n",
    "    'depends_on_past': False,  # Запуск DAG зависит ли от успешности окончания предыдущего запуска по расписанию\n",
    "}\n",
    "\n",
    "\n",
    "def get_path(file_name):\n",
    "    return os.path.join(os.path.expanduser('~'), file_name)\n",
    "\n",
    "\n",
    "def download_titanic_dataset():\n",
    "    url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
    "    df = pd.read_csv(url)\n",
    "    df.to_csv(get_path('titanic.csv'), encoding='utf-8')\n",
    "\n",
    "\n",
    "def pivot_dataset():\n",
    "    titanic_df = pd.read_csv(get_path('titanic.csv'))\n",
    "    df = titanic_df.pivot_table(index=['Sex'],\n",
    "                                columns=['Pclass'],\n",
    "                                values='Name',\n",
    "                                aggfunc='count').reset_index()\n",
    "    df.to_csv(get_path('titanic_pivot.csv'))\n",
    "    \n",
    "# В контексте DAG'а зададим набор task'ок\n",
    "# Объект-инстанс Operator'а - это и есть task\n",
    "with DAG(\n",
    "        dag_id='titanic_pivot',  # Имя DAG\n",
    "        schedule_interval=None,  # Периодичность запуска, например, \"00 15 * * *\"\n",
    "        default_args=args,  # Базовые аргументы\n",
    ") as dag:\n",
    "    # BashOperator, выполняющий указанную bash-команду\n",
    "    first_task = BashOperator(\n",
    "        task_id='first_task',\n",
    "        bash_command='echo \"Here we start! Info: run_id={{ run_id }} | dag_run={{ dag_run }}\"',\n",
    "        dag=dag,\n",
    "    )\n",
    "    # Загрузка датасета\n",
    "    create_titanic_dataset = PythonOperator(\n",
    "        task_id='download_titanic_dataset',\n",
    "        python_callable=download_titanic_dataset,\n",
    "        dag=dag,\n",
    "    )\n",
    "    # Чтение, преобразование и запись датасета\n",
    "    pivot_titanic_dataset = PythonOperator(\n",
    "        task_id='pivot_dataset',\n",
    "        python_callable=pivot_dataset,\n",
    "        dag=dag,\n",
    "    )\n",
    "    # Порядок выполнения тасок\n",
    "    first_task >> create_titanic_dataset >> pivot_titanic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные моменты:\n",
    "\n",
    "* В DAG'е у нас используются 2 PythonOperator и 1 BashOperator. Обратите внимание, что PythonOperator'ы принимают функцию, которую необходимы выполнить. В первом случае это download_titanic_dataset, которая скачивает датасет из сети, во втором случае это pivot_dataset, которая сохраняет сводную таблицу из исходного файла (сохраненного предыдущей функцией).\n",
    "* В schedule_interval время задается в UTC, то есть нужно помнить о разнице часовых поясов, при этом в UI для удобства можно выбрать часовой пояс и даты будут отображаться соответственно (появилось в версии [1.10.10](https://airflow.apache.org/blog/airflow-1.10.10/#allow-user-to-chose-timezone-to-use-in-the-rbac-ui)); расписание может указываться в виде строки (str), объекта datetime.timedelta или в формате cron.\n",
    "* В Airflow допустимы конструкции >> и << (перегрузка операторов битового сдвига), а также методы .set_upstream и .set_downstream, для описания зависимости между двумя операторами, а именно порядка выполнения тасок; можно также таски задавать списком: t1 >> \\[t2, t3\\].\n",
    "* В bash_command использованы конструкции вида {{ run_id }}, это __шаблоны Jinja__ - [Jinja Templating](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#jinja-templating), они работают на основе Python-библиотеки для рендеринга шаблонов [Jinja](https://jinja.palletsprojects.com/en/master/), которая компилирует каждый шаблон в Python executable, который принимает на вход контекст и возвращает строку — отрендеренный шаблон. Вместе с [Macros](https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html)'ами и Variables (переменные рассмотрим в след. уроке) они позволяют удобно использовать в разработке различные переменные и форматировать их."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующие конструкции равносильны:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create_titanic_dataset >> pivot_titanic_dataset\n",
    "pivot_titanic_dataset.set_upstream(create_titanic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметров DAG много больше, более подробно можно прочитать в [документации](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html).\n",
    "\n",
    "Далее готовый файл можно протестировать, проверив корректность кода, выполнив:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python3 dag.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ошибок нет, то далее файл с DAG'ом нужно поместить в папку, в которую \"смотрит\" scheduler - $AIRFLOW_HOME/dags, причем с определенной периодичностью, дефолтно 300 сек - __каждые 5 минут__, за это отвечает параметр в секции \\[scheduler\\] - __dag_dir_list_interval__.\n",
    "\n",
    "Добавить DAG можно вручную скопировав \\*.py файл, либо следующей командой:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cp dags/first_dag.py `airflow config get-value core dags_folder`/first_dag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если всё сделано верно, то в списке появится наш DAG. Его можно активировать по кнопке __Pause/Unpause DAG__ и запустить - __Trigger Dag__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2/first_dag.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2/trigger_dag.png\" style=\"width: 1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таски по очереди перейдут в статус __\"success\"__ и после выполнения всех тасок результат работы DAG'а можно посмотреть в файле, расположенного на вашем локальном компьютере по пути: __~/titanic_pivot.csv__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2/dag_result.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Познакомимся детальнее с Airflow WebUI, рассмотрим интерфейс DAG:\n",
    "\n",
    "* Представления DAG: __Tree View__ и __Graph View__\n",
    "* Время выполнения и попытки тасок: __Task Duration__ и __Task Tries__\n",
    "* Общее время выполнения DAG: __Landing Times__\n",
    "* Диаграмма Ганта, показывающая время исполнения тасок в интервалах: __Gantt__\n",
    "* Детали DAG'а: __Details__\n",
    "* Текущий код DAG'а: __Code__\n",
    "\n",
    "Посмотрев вкладку DAGs, изучим другие вкладки:\n",
    "\n",
    "* Security: пользователи, поли, статистика, доступы\n",
    "* Browse: списки DAG Runs / Jobs / Tasks, Logs\n",
    "* Admin: Variables, Configurations, Connections, Plugins, Pools, XComs\n",
    "* Docs: ссылки на офф. документацию и Git-репозиторий\n",
    "* Profile: данные профиля юзера\n",
    "\n",
    "Скриншоты с основными вкладками WebUI также есть в [документации](https://airflow.apache.org/docs/apache-airflow/stable/ui.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, интерфейс Airflow предлагает множество полезных функций и наглядно визуализирует все процессы. Пример с нашим DAG'ом довольно маленький, в реальном проекте DAG'и достигают десятков или даже сотен тасок:\n",
    "\n",
    "<img src=\"images/2/dag_view.png\" style=\"width: 800px;\"> \n",
    "\n",
    "Инстанс DAG'а в определенный момент времени называется __DAG Run__. Каждый DAG может как иметь расписание, так и не иметь, которое показывает, как DAG Run был создан."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Мониторить__ процесс исполнения задачи можно по __логу__, доступному в TaskInstance -> Log, причем логи разбиты по попыткам (Log by attempts). Для задач, исполняемых на кластере, например, при spark-submit, лог особенно интересен, так как там можно посмотреть __application id__ на кластере Hadoop и по этому id посмотреть детали задачи уже в UI кластера (All Application). Также в TaskInstance есть Task Actions для перезапуска тасок и изменения статуса.\n",
    "\n",
    "<img src=\"images/2/taskinstance.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow сравнительно не трудно развернуть с нуля и установить PostgreSQL в качестве метабазы вместо SQLite. У Airflow много зависимостей, поэтому нужно быть внимательнее с потенциальными конфликтами. Для полноценной работы Airflow помимо базы нужно запустить webserver и scheduler. Наглядный Web UI позволяет обозревать существующие паплайны, запускать их и мониторить. Scheduler следит за появлением новых DAG'ов (в $AIRFLOW_HOME/dags) и отображает их в UI по мере появления.\n",
    "\n",
    "Airflow комплексный инструмент, предлагающий дата-инжинеру множество удобных возможностей для самых разных задач и хорошо подходит для классической задачи ETL:\n",
    "\n",
    "<img src=\"images/2/airflow_etl.png\" style=\"width: 600px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [PostgreSQL Client Applications](https://www.postgresql.org/docs/13/app-psql.html)\n",
    "2. [PostgreSQL Server Applications](https://www.postgresql.org/docs/13/app-postgres.html)\n",
    "3. [Введение в Apache Airflow](https://khashtamov.com/ru/apache-airflow-introduction/)\n",
    "4. [Как мы оркестрируем процессы обработки данных с помощью Apache Airflow](https://habr.com/ru/company/lamoda/blog/518620/)\n",
    "5. [Airflow — инструмент, чтобы удобно и быстро разрабатывать и поддерживать batch-процессы обработки данных](https://habr.com/ru/company/mailru/blog/339392/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.__ Установить и настроить Airflow по материалам лекции, создать проект в PyCharm, запустить пример DAG'а из лекции на локальном компьютере и убедиться в успешном расчете пайплайна.\n",
    "\n",
    "__2.__ Написать функцию __mean_fare_per_class()__, которая считывает файл titanic.csv и расчитывает среднюю арифметическую цену билета (Fare) для каждого класса (Pclass) и сохраняет результирующий датафрейм в файл __titanic_mean_fares.csv__\n",
    "\n",
    "__3.__ Добавить в DAG таск с названием __mean_fares_titanic_dataset__, который будет исполнять функцию mean_fare_per_class(), причем эта задача должна запускаться в параллель с pivot_titanic_dataset после таски create_titanic_dataset.\n",
    "\n",
    "__4.__ В конец пайплайна (после завершения тасок pivot_titanic_dataset и mean_fares_titanic_dataset) добавить шаг с названием __last_task__, на котором в STDOUT выводится строка, сообщающая об окончании расчета и выводящая execution date в формате YYYY-MM-DD. Пример строки: \"Pipeline finished! Execution date is 2020-12-28\"\n",
    "\n",
    "__Формат сдачи д/з__: приложите ссылку на ваш Git с кодом DAG'а и туда же загрузите два скриншота с:\n",
    "\n",
    "* выводом комманды в консоли \"head ~/titanic_mean_fares.csv\"\n",
    "* содержанием лога инстанса таски last_task в UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
